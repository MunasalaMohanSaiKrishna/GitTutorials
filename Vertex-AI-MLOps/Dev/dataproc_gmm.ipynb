{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de213ad6-b078-441a-9a54-8325a3b9fead",
   "metadata": {},
   "source": [
    "# Dataproc Spark Job\n",
    "- Dataproc Cluster\n",
    "- Job with BQ data\n",
    "- Delete Dataproc Cluster\n",
    "\n",
    "API Reference: https://googleapis.dev/python/dataproc/0.7.0/gapic/v1/api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae060b7-d4e5-444d-9f37-c5a14590756b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9677d-9213-422f-ac7d-f47ebacd690b",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c09987d-4e28-4e8c-958d-f94e17945c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID='statmike-mlops'\n",
    "DATANAME = 'fraud'\n",
    "NOTEBOOK = 'dataproc'\n",
    "\n",
    "DATAPROC_COMPUTE = \"n1-standard-4\"\n",
    "DATAPROC_MAIN_INSTANCES = 1\n",
    "DATAPROC_WORK_INSTANCES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619febb2-e2b4-40d9-846e-6d71b4aaf273",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c8b0c79-2668-49aa-8ca1-c2f943ff4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import dataproc_v1\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341a755-9b3d-496f-a0ba-9bb296fe34bc",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b817d332-40c5-4336-8895-85760b4da0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": f\"{REGION}-dataproc.googleapis.com:443\"}\n",
    "clients = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0d7a50-5e65-429e-8b8d-78228aba232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients['cluster'] = dataproc_v1.ClusterControllerClient(client_options = client_options)\n",
    "clients['job'] = dataproc_v1.JobControllerClient(client_options = client_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0855e5-9e91-44da-9452-6de8e888ab85",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "020d0e09-1d61-470d-b952-344ee553d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83ab29-df43-4713-a258-22ce54dfbd2a",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "362f7151-8174-4c2e-81f7-9b9fa9eeee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64423b-36a3-4f16-875c-229b9c88f087",
   "metadata": {},
   "source": [
    "## Define Job\n",
    "- https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml#run_a_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43c2c9f2-3078-46f3-b572-e672a074662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp/dataproc/gm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/gm.py\n",
    "from __future__ import print_function\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.sql.session import SparkSession\n",
    "# The imports, above, allow us to access SparkML features specific to linear\n",
    "# regression as well as the Vectors types.\n",
    "\n",
    "\n",
    "# Define a function that collects the features of interest\n",
    "# (mother_age, father_age, and gestation_weeks) into a vector.\n",
    "# Package the vector in a tuple containing the label (`weight_pounds`) for that\n",
    "# row.\n",
    "def vector_from_inputs(r):\n",
    "  return (r[\"weight_pounds\"], Vectors.dense(float(r[\"mother_age\"]),\n",
    "                                            float(r[\"father_age\"]),\n",
    "                                            float(r[\"gestation_weeks\"]),\n",
    "                                            float(r[\"weight_gain_pounds\"]),\n",
    "                                            float(r[\"apgar_5min\"])))\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#temp space for bq export used by connector\n",
    "spark.conf.set('temporaryGcsBucket',\"statmike-mlops\")\n",
    "\n",
    "# Read the data from BigQuery as a Spark Dataframe.\n",
    "natality_data = spark.read.format(\"bigquery\").option(\"table\", \"bigquery-public-data.samples.natality\").load()\n",
    "# Create a view so that Spark SQL queries can be run against the data.\n",
    "natality_data.createOrReplaceTempView(\"natality\")\n",
    "\n",
    "# subset data rows and columns\n",
    "sql_query = \"\"\"\n",
    "SELECT weight_pounds, mother_age, father_age, gestation_weeks, weight_gain_pounds, apgar_5min\n",
    "from natality\n",
    "where weight_pounds is not null\n",
    "and mother_age is not null\n",
    "and father_age is not null\n",
    "and gestation_weeks is not null\n",
    "and weight_gain_pounds is not null\n",
    "and apgar_5min is not null\n",
    "\"\"\"\n",
    "clean_data = spark.sql(sql_query)\n",
    "\n",
    "# Create an input DataFrame for Spark ML using the above function.\n",
    "training_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\", \"features\"])\n",
    "training_data.cache()\n",
    "\n",
    "# cluster the feature rows with GM\n",
    "gm = GaussianMixture().setK(4).setSeed(1234567)\n",
    "model = gm.fit(training_data)\n",
    "\n",
    "# write data to BigQuery\n",
    "model.gaussiansDF.write.format('bigquery').option(\"table\", \"statmike-mlops.fraud.gm_cluster\").mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f432cfc-2027-4871-938c-016b172c6155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://temp/dataproc/gm.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  2.1 KiB/  2.1 KiB]                                                \n",
      "Operation completed over 1 objects/2.1 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {DIR}/gm.py {URI}/{TIMESTAMP}/gm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d5e07-a825-41f4-b587-348af1d5ed62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method 1: Submit Serverless (Batch) Dataproc Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bc890-8b4d-49c4-b296-3791b51452ad",
   "metadata": {},
   "source": [
    "During Private Preview: need to allowlist the project and user...\n",
    "\n",
    "Note: Dataproc Serveless requires a subnet with Private Google Access. The first three cells below check for the private access, enable private access, check again to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa23eb3-06a9-48ec-94ba-769dbf8efa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks subnets describe default --region={REGION} --format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52fa5f66-09c3-4d4e-8ed2-102ed584a095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated [https://www.googleapis.com/compute/v1/projects/statmike-mlops/regions/us-central1/subnetworks/default].\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks subnets update default --region={REGION} --enable-private-ip-google-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "474963f5-f2e1-4cdd-86d3-e559bdda232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks subnets describe default --region={REGION} --format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6e7bc2f-12ae-4211-89b6-c7ae8156bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [d3395366a410496987b5cdccb966fa26] submitted.\n",
      "Using the default image serverless-spark-default:2.1\n",
      "CONDA_HOME=/opt/dataproc/opt/conda/default\n",
      "PYSPARK_PYTHON=/opt/dataproc/opt/conda/default/bin/python\n",
      "Warning: Ignoring non-Spark config property: dataproc.batch.uuid\n",
      "Warning: Ignoring non-Spark config property: dataproc.batch.id\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "Batch [d3395366a410496987b5cdccb966fa26] finished.\n",
      "metadata:\n",
      "  '@type': type.googleapis.com/google.cloud.dataproc.v1.BatchOperationMetadata\n",
      "  batch: projects/statmike-mlops/locations/us-central1/batches/d3395366a410496987b5cdccb966fa26\n",
      "  batchUuid: 29ff1546-039b-4c10-a3f7-386e3afd4502\n",
      "  createTime: '2021-10-21T18:40:15.621906Z'\n",
      "  description: Batch\n",
      "  operationType: BATCH\n",
      "name: projects/statmike-mlops/regions/us-central1/operations/fbaa0995-8859-314c-a065-baf30487edd1\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta dataproc batches submit pyspark {DIR}/gm.py --project={PROJECT_ID} --region={REGION} --deps-bucket={BUCKET} --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: User Managed Dataproc Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bdad4-87ab-4676-948c-50bcbee8e5e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Cluster\n",
    "https://cloud.google.com/dataproc/docs/guides/create-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda13ab0-6a55-409f-860d-42e3694ca0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_specs = {\n",
    "\t\"project_id\": PROJECT_ID,\n",
    "    \"cluster_name\": DATANAME,\n",
    "    \"config\": {\n",
    "    \t\"master_config\": {\"num_instances\": DATAPROC_MAIN_INSTANCES, \"machine_type_uri\": DATAPROC_COMPUTE},\n",
    "    \t\"worker_config\": {\"num_instances\": DATAPROC_WORK_INSTANCES, \"machine_type_uri\": DATAPROC_COMPUTE}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50fcb6e3-4a1d-451f-860d-a2a2a1700e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = clients['cluster'].create_cluster(\n",
    "    request = {\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"cluster\": cluster_specs\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2c137d0-c51c-47fb-890b-bae5cb5cac1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fraud'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.result().cluster_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe55298-408c-4d9d-803a-14b249d8bae7",
   "metadata": {},
   "source": [
    "### Submit Job\n",
    "- https://cloud.google.com/dataproc/docs/samples/dataproc-submit-pyspark-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "926962cd-ddd6-4446-843a-a34381ccefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_specs = {\n",
    "\t\"placement\": {\"cluster_name\": DATANAME},\n",
    "    \"pyspark_job\": {\n",
    "    \t\"main_python_file_uri\": f\"{URI}/{TIMESTAMP}/gm.py\",\n",
    "        \"jar_file_uris\": [\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19befe09-675f-4687-9ce3-63fcd6f54dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = clients['job'].submit_job(project_id = PROJECT_ID, region = REGION, job = job_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41c42366-1798-4921-a20e-934fce4349c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'84c95c0b-cb32-4363-a1a6-641bf1559108'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.reference.job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748db2c2-199e-4be6-b9fa-3b96b45c3908",
   "metadata": {},
   "source": [
    "### Wait On Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "507dedc9-bd5a-4e58-b084-6229b16c4e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ljob = clients['job'].get_job(project_id = PROJECT_ID, region = REGION, job_id = job.reference.job_id)\n",
    "    if ljob.status.state.name == \"ERROR\":\n",
    "        raise Exception(ljob.status.details)\n",
    "    elif ljob.status.state.name == \"DONE\":\n",
    "        print (\"Finished\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc628d-3d51-4b6b-b17f-6ca79fb9de22",
   "metadata": {},
   "source": [
    "### Review Results\n",
    "- Go to BiqQuery and review the output table: statmike-mlops.fraud.gm_cluster in my case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b854ed0e-2567-4133-9e1d-bb06c4995694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference {\n",
       "  project_id: \"statmike-mlops\"\n",
       "  job_id: \"84c95c0b-cb32-4363-a1a6-641bf1559108\"\n",
       "}\n",
       "placement {\n",
       "  cluster_name: \"fraud\"\n",
       "  cluster_uuid: \"80c51baf-d595-4301-a08b-44e799f638ac\"\n",
       "}\n",
       "pyspark_job {\n",
       "  main_python_file_uri: \"gs://statmike-mlops/fraud/models/dataproc/20211021183942/gm.py\"\n",
       "  jar_file_uris: \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"\n",
       "}\n",
       "status {\n",
       "  state: DONE\n",
       "  state_start_time {\n",
       "    seconds: 1634896929\n",
       "    nanos: 78067000\n",
       "  }\n",
       "}\n",
       "yarn_applications {\n",
       "  name: \"gm.py\"\n",
       "  state: FINISHED\n",
       "  progress: 1.0\n",
       "  tracking_url: \"http://fraud-m:8088/proxy/application_1634850985280_0002/\"\n",
       "}\n",
       "status_history {\n",
       "  state: PENDING\n",
       "  state_start_time {\n",
       "    seconds: 1634894938\n",
       "    nanos: 741183000\n",
       "  }\n",
       "}\n",
       "status_history {\n",
       "  state: SETUP_DONE\n",
       "  state_start_time {\n",
       "    seconds: 1634894938\n",
       "    nanos: 762715000\n",
       "  }\n",
       "}\n",
       "status_history {\n",
       "  state: RUNNING\n",
       "  details: \"Agent reported job success\"\n",
       "  state_start_time {\n",
       "    seconds: 1634894938\n",
       "    nanos: 928553000\n",
       "  }\n",
       "}\n",
       "driver_control_files_uri: \"gs://dataproc-staging-us-central1-691911073727-lnrwidqq/google-cloud-dataproc-metainfo/80c51baf-d595-4301-a08b-44e799f638ac/jobs/84c95c0b-cb32-4363-a1a6-641bf1559108/\"\n",
       "driver_output_resource_uri: \"gs://dataproc-staging-us-central1-691911073727-lnrwidqq/google-cloud-dataproc-metainfo/80c51baf-d595-4301-a08b-44e799f638ac/jobs/84c95c0b-cb32-4363-a1a6-641bf1559108/driveroutput\"\n",
       "job_uuid: \"84c95c0b-cb32-4363-a1a6-641bf1559108\"\n",
       "done: true"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ljob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee539b-f3b1-469a-927e-5ba87c1bf754",
   "metadata": {},
   "source": [
    "### Delete Cluster\n",
    "https://cloud.google.com/dataproc/docs/guides/manage-cluster#delete_a_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ba85676-ae7e-47be-98ae-20503b366e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "delCluster = clients['cluster'].delete_cluster(\n",
    "    request = {\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"cluster_name\": cluster.result().cluster_name\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92cdcf-28e3-4b0c-92d5-3a56f63d8d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
