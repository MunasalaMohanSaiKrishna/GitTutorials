{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8c3a03-fb3b-4bde-83dd-98c037c1dea9",
   "metadata": {},
   "source": [
    "# Dataproc Spark Job\n",
    "- Dataproc Cluster\n",
    "- Job with BQ data\n",
    "- Delete Dataproc Cluster\n",
    "\n",
    "API Reference: https://googleapis.dev/python/dataproc/0.7.0/gapic/v1/api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044111b-05e0-4af0-b8f4-93c70b4aaeb4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dde0f5-f55c-4e3c-9664-4b61704f0150",
   "metadata": {},
   "source": [
    "Enable the Dataproc API: only needed once for a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e6065f-0921-41a9-af58-7975dbd10990",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable dataproc.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76eb947-04a8-417e-837a-e67bb8524f6e",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a112ce96-07e7-44f4-ba8c-e0814c1913fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID='statmike-demo3'\n",
    "DATANAME = 'fraud'\n",
    "NOTEBOOK = 'dataproc'\n",
    "\n",
    "DATAPROC_COMPUTE = \"n1-standard-4\"\n",
    "DATAPROC_MAIN_INSTANCES = 1\n",
    "DATAPROC_WORK_INSTANCES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ae854-db88-4b7f-b574-67357d159bbd",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb913f5-f53a-406d-89c4-e4bb2e60e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2308cb2-8f80-4fa1-98f3-91091bbd5b49",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6662a84c-fa6f-4cac-b489-c10e0ff67976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca034f7-4f73-4098-ac66-ea013d02be87",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c643430a-d092-4ad2-ad24-9da63cb8cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bdeee6-b4fe-4153-8149-103fd0fa5851",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43d776e-ff33-4e78-be0b-540c8cc3feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681d5b2-b5e4-43bd-b271-7048061b4dd9",
   "metadata": {},
   "source": [
    "## Define Job\n",
    "- https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml#run_a_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6fbe3c-dc59-4705-8b02-81bd22831f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp/dataproc/lr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/lr.py\n",
    "from __future__ import print_function\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "# Define a function that collects the features of interest\n",
    "def vector_from_inputs(r):\n",
    "  return (r[\"Class\"], Vectors.dense(float(r[\"Amount\"]),\n",
    "                                    int(r[\"Time\"]),\n",
    "                                    float(r[\"V1\"]),\n",
    "                                    float(r[\"V2\"]),\n",
    "                                    float(r[\"V3\"]),\n",
    "                                    float(r[\"V4\"]),\n",
    "                                    float(r[\"V5\"]),\n",
    "                                    float(r[\"V6\"]),\n",
    "                                    float(r[\"V7\"]),\n",
    "                                    float(r[\"V8\"]),\n",
    "                                    float(r[\"V9\"]),\n",
    "                                    float(r[\"V10\"]),\n",
    "                                    float(r[\"V11\"]),\n",
    "                                    float(r[\"V12\"]),\n",
    "                                    float(r[\"V13\"]),\n",
    "                                    float(r[\"V14\"]),\n",
    "                                    float(r[\"V15\"]),\n",
    "                                    float(r[\"V16\"]),\n",
    "                                    float(r[\"V17\"]),\n",
    "                                    float(r[\"V18\"]),\n",
    "                                    float(r[\"V19\"]),\n",
    "                                    float(r[\"V20\"]),\n",
    "                                    float(r[\"V21\"]),\n",
    "                                    float(r[\"V22\"]),\n",
    "                                    float(r[\"V23\"]),\n",
    "                                    float(r[\"V24\"]),\n",
    "                                    float(r[\"V25\"]),\n",
    "                                    float(r[\"V26\"]),\n",
    "                                    float(r[\"V27\"]),\n",
    "                                    float(r[\"V28\"])\n",
    "                                   ),\n",
    "          r['splits'], r['transaction_id']\n",
    "         )\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#temp space for bq export used by connector\n",
    "spark.conf.set('temporaryGcsBucket',\"statmike-demo3\")\n",
    "\n",
    "# Read the data from BigQuery as a Spark Dataframe.\n",
    "input_data = spark.read.format(\"bigquery\").option(\"table\", \"statmike-demo3.fraud.fraud_prepped\").load()\n",
    "input_data.createOrReplaceTempView(\"fraud\")\n",
    "\n",
    "# subset data rows and columns\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "from fraud \n",
    "\"\"\"\n",
    "clean_data = spark.sql(sql_query) \n",
    "\n",
    "# Create an input DataFrame for Spark ML using the above function.\n",
    "all_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\", \"features\", \"splits\", \"transactions_id\"])\n",
    "all_data.cache()\n",
    "\n",
    "# logistic regression with pyspark.ml\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter = 20)\n",
    "lrModel = lr.fit(all_data.filter(df.splits=='TRAIN'))\n",
    "predictions = lrModel.transform(all_data)\n",
    "\n",
    "# write data to BigQuery\n",
    "predictions.write.format('bigquery').option(\"table\", \"statmike-demo3.fraud.dataproc_lr\").mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d90a0d8-10bf-4a1d-8636-916d7e6d0481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://temp/dataproc/lr.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
      "Operation completed over 1 objects/2.9 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {DIR}/lr.py {URI}/{TIMESTAMP}/lr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab80b8-2756-4f80-a271-0c03db942f98",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submit Serverless (Batch) Dataproc Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae00912-0413-4851-8c66-ff6302ce112d",
   "metadata": {},
   "source": [
    "During Private Preview: need to allowlist the project and user...\n",
    "\n",
    "Note: Dataproc Serveless requires a subnet with Private Google Access. The first three cells below check for the private access, enable private access, check again to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d543c5f-3c86-47a4-8934-b36f522942c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks subnets describe default --region={REGION} --format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3e44b2-d8b4-41d6-9de9-5b381c7893f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated [https://www.googleapis.com/compute/v1/projects/statmike-demo3/regions/us-central1/subnetworks/default].\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks subnets update default --region={REGION} --no-enable-private-ip-google-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69703c47-ee61-4b51-8ee1-96a75ccbeb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks subnets describe default --region={REGION} --format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b21a31f-eee3-41dc-bcaf-c4df8c6e755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [b73beb67bed74b7aa7a7c2d39fffef11] submitted.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.batches.submit.pyspark) Batch job is FAILED. Detail: Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.\n",
      "Running auto diagnostics on the batch. It may take few minutes before diagnostics output is available. Please check diagnostics output by running 'gcloud dataproc batches describe' command.\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc batches submit pyspark {DIR}/lr.py --project={PROJECT_ID} --region={REGION} --deps-bucket={BUCKET} --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437155e-31fb-4c3c-a0ca-b0ba14fa721e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
