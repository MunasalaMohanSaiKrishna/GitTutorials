{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de213ad6-b078-441a-9a54-8325a3b9fead",
   "metadata": {},
   "source": [
    "# Dataproc Spark Job\n",
    "- Dataproc Cluster\n",
    "- Job with BQ data\n",
    "- Delete Dataproc Cluster\n",
    "\n",
    "API Reference: https://googleapis.dev/python/dataproc/0.7.0/gapic/v1/api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae060b7-d4e5-444d-9f37-c5a14590756b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9677d-9213-422f-ac7d-f47ebacd690b",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c09987d-4e28-4e8c-958d-f94e17945c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID='statmike-mlops'\n",
    "DATANAME = 'fraud'\n",
    "NOTEBOOK = 'dataproc'\n",
    "\n",
    "DATAPROC_COMPUTE = \"n1-standard-4\"\n",
    "DATAPROC_MAIN_INSTANCES = 1\n",
    "DATAPROC_WORK_INSTANCES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619febb2-e2b4-40d9-846e-6d71b4aaf273",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c8b0c79-2668-49aa-8ca1-c2f943ff4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import dataproc_v1\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341a755-9b3d-496f-a0ba-9bb296fe34bc",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b817d332-40c5-4336-8895-85760b4da0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": f\"{REGION}-dataproc.googleapis.com:443\"}\n",
    "clients = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b0d7a50-5e65-429e-8b8d-78228aba232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients['cluster'] = dataproc_v1.ClusterControllerClient(client_options = client_options)\n",
    "clients['job'] = dataproc_v1.JobControllerClient(client_options = client_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0855e5-9e91-44da-9452-6de8e888ab85",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "020d0e09-1d61-470d-b952-344ee553d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83ab29-df43-4713-a258-22ce54dfbd2a",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "362f7151-8174-4c2e-81f7-9b9fa9eeee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1004 08:58:28.045879212     133 backup_poller.cc:133]       Run client channel backup poller: {\"created\":\"@1633337908.045714837\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1633337908.045707029\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":957,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    }
   ],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bdad4-87ab-4676-948c-50bcbee8e5e5",
   "metadata": {},
   "source": [
    "## Create Cluster\n",
    "https://cloud.google.com/dataproc/docs/guides/create-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cda13ab0-6a55-409f-860d-42e3694ca0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_specs = {\n",
    "\t\"project_id\": PROJECT_ID,\n",
    "    \"cluster_name\": DATANAME,\n",
    "    \"config\": {\n",
    "    \t\"master_config\": {\"num_instances\": DATAPROC_MAIN_INSTANCES, \"machine_type_uri\": DATAPROC_COMPUTE},\n",
    "    \t\"worker_config\": {\"num_instances\": DATAPROC_WORK_INSTANCES, \"machine_type_uri\": DATAPROC_COMPUTE}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50fcb6e3-4a1d-451f-860d-a2a2a1700e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = clients['cluster'].create_cluster(\n",
    "    request = {\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"cluster\": cluster_specs\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2c137d0-c51c-47fb-890b-bae5cb5cac1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fraud'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.result().cluster_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64423b-36a3-4f16-875c-229b9c88f087",
   "metadata": {},
   "source": [
    "## Define Job\n",
    "- https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml#run_a_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "beb6da42-a197-48c9-a69f-2645f7489c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp/dataproc/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/train.py\n",
    "from __future__ import print_function\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.session import SparkSession\n",
    "# The imports, above, allow us to access SparkML features specific to linear\n",
    "# regression as well as the Vectors types.\n",
    "\n",
    "\n",
    "# Define a function that collects the features of interest\n",
    "# (mother_age, father_age, and gestation_weeks) into a vector.\n",
    "# Package the vector in a tuple containing the label (`weight_pounds`) for that\n",
    "# row.\n",
    "def vector_from_inputs(r):\n",
    "  return (r[\"weight_pounds\"], Vectors.dense(float(r[\"mother_age\"]),\n",
    "                                            float(r[\"father_age\"]),\n",
    "                                            float(r[\"gestation_weeks\"]),\n",
    "                                            float(r[\"weight_gain_pounds\"]),\n",
    "                                            float(r[\"apgar_5min\"])))\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "spark.conf.set('temporaryGcsBucket',\"statmike-mlops\")\n",
    "\n",
    "# Read the data from BigQuery as a Spark Dataframe.\n",
    "#natality_data = spark.read.format(\"bigquery\").option(\"table\", \"natality_regression.regression_input\").load()\n",
    "natality_data = spark.read.format(\"bigquery\").option(\"table\", \"bigquery-public-data.samples.natality\").load()\n",
    "    # Create a view so that Spark SQL queries can be run against the data.\n",
    "natality_data.createOrReplaceTempView(\"natality\")\n",
    "\n",
    "# As a precaution, run a query in Spark SQL to ensure no NULL values exist.\n",
    "sql_query = \"\"\"\n",
    "SELECT weight_pounds, mother_age, father_age, gestation_weeks, weight_gain_pounds, apgar_5min\n",
    "from natality\n",
    "where weight_pounds is not null\n",
    "and mother_age is not null\n",
    "and father_age is not null\n",
    "and gestation_weeks is not null\n",
    "and weight_gain_pounds is not null\n",
    "and apgar_5min is not null\n",
    "\"\"\"\n",
    "clean_data = spark.sql(sql_query)\n",
    "\n",
    "# Create an input DataFrame for Spark ML using the above function.\n",
    "training_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\",\n",
    "                                                             \"features\"])\n",
    "training_data.cache()\n",
    "\n",
    "# Construct a new LinearRegression object and fit the training data.\n",
    "lr = LinearRegression(maxIter=5, regParam=0.2, solver=\"normal\")\n",
    "model = lr.fit(training_data)\n",
    "# Print the model summary.\n",
    "print(\"Coefficients:\" + str(model.coefficients))\n",
    "print(\"Intercept:\" + str(model.intercept))\n",
    "print(\"R^2:\" + str(model.summary.r2))\n",
    "model.summary.residuals.show()\n",
    "\n",
    "# write data to BigQuery\n",
    "model.summary.residuals.write.format('bigquery').option(\"table\", \"statmike-mlops.fraud.dataproc\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01d92221-13a9-4d1d-92be-e09a00c7061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://temp/dataproc/train.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  2.5 KiB/  2.5 KiB]                                                \n",
      "Operation completed over 1 objects/2.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {DIR}/train.py {URI}/{TIMESTAMP}/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe55298-408c-4d9d-803a-14b249d8bae7",
   "metadata": {},
   "source": [
    "## Submit Job\n",
    "- https://cloud.google.com/dataproc/docs/samples/dataproc-submit-pyspark-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "926962cd-ddd6-4446-843a-a34381ccefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_specs = {\n",
    "\t\"placement\": {\"cluster_name\": DATANAME},\n",
    "    \"pyspark_job\": {\n",
    "    \t\"main_python_file_uri\": f\"{URI}/{TIMESTAMP}/train.py\",\n",
    "        \"jar_file_uris\": [\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "19befe09-675f-4687-9ce3-63fcd6f54dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = clients['job'].submit_job(project_id = PROJECT_ID, region = REGION, job = job_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "41c42366-1798-4921-a20e-934fce4349c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'34d2b207-bc6b-4307-ba0b-5c8b6fce0319'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.reference.job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748db2c2-199e-4be6-b9fa-3b96b45c3908",
   "metadata": {},
   "source": [
    "## Wait On Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "507dedc9-bd5a-4e58-b084-6229b16c4e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ljob = clients['job'].get_job(project_id = PROJECT_ID, region = REGION, job_id = job.reference.job_id)\n",
    "    if ljob.status.state.name == \"ERROR\":\n",
    "        raise Exception(ljob.status.details)\n",
    "    elif ljob.status.state.name == \"DONE\":\n",
    "        print (\"Finished\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc628d-3d51-4b6b-b17f-6ca79fb9de22",
   "metadata": {},
   "source": [
    "## Review Results\n",
    "- Go to BiqQuery and review the output table: statmike-mlops.fraud.gm_cluster in my case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854ed0e-2567-4133-9e1d-bb06c4995694",
   "metadata": {},
   "outputs": [],
   "source": [
    "ljob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee539b-f3b1-469a-927e-5ba87c1bf754",
   "metadata": {},
   "source": [
    "## Delete Cluster\n",
    "https://cloud.google.com/dataproc/docs/guides/manage-cluster#delete_a_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9ba85676-ae7e-47be-98ae-20503b366e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "delCluster = clients['cluster'].delete_cluster(\n",
    "    request = {\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"cluster_name\": cluster.result().cluster_name\n",
    "\t}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "managed-notebooks.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m80"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
