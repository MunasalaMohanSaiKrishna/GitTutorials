{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f8ff0c-39dd-4b5f-ad2a-620155b03761",
   "metadata": {},
   "source": [
    "# 05 - Vertex AI Custom Model - TensorFlow - Notebook to Hyperparameter Tuning Script\n",
    "\n",
    "In the notebook [05 - Vertex AI Custom Model - TensorFlow - in Notebook](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20in%20Notebook.ipynb) code was developed ad-hoc and run piece by piece in cells of the notebook.  As the data scales and training scales it is tempting to increase the size of compute backing the notebook environment with more CPU, Memory and/or GPU.  A better practice for scaling is running the ML training code in training jobs with specified compute.  The notebooks `05a` through `05i` show the ways to run training jobs for various purposes.  The notebook [05 - Vertex AI Custom Model - TensorFlow - Notebook to Script](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb) created a training script that is used by notebooks `05a` through `05f` and is the basis for the hyperparameter tuning version built here for use in `05g` through `05i`.\n",
    "\n",
    "The script created below is additive to the one created in [05 - Vertex AI Custom Model - TensorFlow - Notebook to Script](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb).  The addition will be called out is section with titles that start with `Hyperparameter Tuning`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b894dd3-9cd0-4661-903b-4eb0cc1d68b8",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a876bd8-fb43-4845-8c49-e5328568c764",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "020d8ae5-3bf8-4f53-b277-91f90ba88717",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = './code/hp_train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebce6e-b526-4938-8c56-4cb641e8e445",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf6d29ac-d6d7-4f2c-a49e-45187ab28a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622aa7f0-25b9-4d2e-a561-e8813a4b3387",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cd8e293-49f3-4660-9262-012573acb726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code directory alredy exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('code'):\n",
    "    print('The code directory alredy exists')\n",
    "else:\n",
    "    print('Creating the code directory')\n",
    "    os.makedirs('code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9843f-d39c-4c4a-896b-665821ab6973",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the ML Training Code\n",
    "\n",
    "Use the cell magic `%%writefile` to create a single ML training code file named `train.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db95040-c3c4-4066-be2c-ac010053d8c3",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d214c49-fd7f-4e4a-a484-d181034af5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SCRIPT_PATH}\n",
    "\n",
    "# package import\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9a132-2839-44f4-ada9-a65a2e3f925a",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Specific Additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b634879-0fe9-4137-8eb9-a16ecbf8d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "import hypertune\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226df855-33ee-4b10-ae0e-37921b6f32e9",
   "metadata": {},
   "source": [
    "### Parse inputs and parameters\n",
    "This script uses the `argparse` package to parse input parameters.  To see more details on this method and alternatives like `docopt` and `click` as well as methods for using files to import parameters (YAML, JSON, pickle) check on this tips notebook: [Python Job Parameters](../Tips/Python%20Job%20Parameters.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6164121f-7994-4bf2-a84b-68f1292573e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
    "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
    "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
    "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
    "parser.add_argument('--series', dest = 'series', type=str)\n",
    "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
    "parser.add_argument('--run_name', dest = 'run_name', type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519083b9-7c2c-4fbc-a9db-b20c46f38bb5",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Specific Additions\n",
    "These are the inputs for the hyperparameters being iterated over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb1fba93-6923-4f9e-b4f2-d8fedfadb79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "# hyperparameters\n",
    "parser.add_argument('--lr', dest='learning_rate', required=True, type=float, help='Learning Rate')\n",
    "parser.add_argument('--m', dest='momentum', required=True, type=float, help='Momentum')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eebfdd1-be18-45e2-bc5d-510857ac07c7",
   "metadata": {},
   "source": [
    "Setup the hyperparameters for TensorBoard's HPARAMS interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bca8e636-2d3b-4114-b99e-29f155fd4aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# setup tensorboard hparams\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.0, 1.0))\n",
    "HP_MOMENTUM = hp.HParam('momentum', hp.RealInterval(0.0,1.0))\n",
    "hparams = {\n",
    "    HP_LEARNING_RATE: args.learning_rate,\n",
    "    HP_MOMENTUM: args.momentum\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f776b-2a74-4c09-983b-0fea7a5b30ec",
   "metadata": {},
   "source": [
    "### Define Clients for Services\n",
    "This script with use clients for BigQuery and Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22f68c3b-11aa-4d72-a37d-903b559ab7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# clients\n",
    "bq = bigquery.Client(project = args.project_id)\n",
    "aiplatform.init(project = args.project_id, location = args.region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b958d5c-ac9a-44dc-9b20-e1a0f50a66a4",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Specific Additions\n",
    "Create a client for the hyperparameter tuning service and use the `trail_id` it returns to make the run name for Vertex AI Experiments unique for this specific set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1bf1b90-adbb-4b16-9270-b0cd9e4d1750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "hpt = hypertune.HyperTune()\n",
    "args.run_name = f'{args.run_name}-{hpt.trial_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2ed3f-01b6-4724-94e2-087d765b12e8",
   "metadata": {},
   "source": [
    "### Setup Vertex AI Experiments\n",
    "\n",
    "Use the Vertex AI client to setup a run of the experiment (input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc8e8520-2a86-4c9e-9eac-e6b66dd4633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# Vertex AI Experiment\n",
    "expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
    "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1529ded-f198-4e90-9209-8b9089b5414d",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Specific Additions\n",
    "Log this trails values for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67bda91e-4c29-4fe8-b9b3-8c62fd7a6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "expRun.log_params({'hyperparameter.learning_rate': args.learning_rate, 'hyperparameter.momentum': args.momentum})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd9974-cef0-4ffc-84a1-8087b13969ad",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "Use the BigQuery client to read column information and target variable information for the source table in BigQuery.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ce3e9f7-0aa4-4e0b-b5f9-a0d1a96d1cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
    "schema = bq.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
    "\n",
    "# Make a list of columns to omit\n",
    "OMIT = args.var_omit + ['splits']\n",
    "\n",
    "# use schema to prepare a list of columns to read from BigQuery\n",
    "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "# all the columns in this data source are either float64 or int64\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed4b72-a00b-41a1-982e-df542b52d7a1",
   "metadata": {},
   "source": [
    "### Read From BigQuery Using TensorFlow I/O \n",
    "\n",
    "Define function for reading from BigQuery using TFIO and for transforming the input data into features and a target variable that is one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2d23786-a212-41ae-bb76-6dbd0bb4bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# remap input data to Tensorflow inputs of features and target\n",
    "def transTable(row_dict):\n",
    "    target = row_dict.pop(args.var_target)\n",
    "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    return(row_dict, target)\n",
    "\n",
    "# function to setup a bigquery reader with Tensorflow I/O\n",
    "def bq_reader(split):\n",
    "    reader = BigQueryClient()\n",
    "\n",
    "    training = reader.read_session(\n",
    "        parent = f\"projects/{args.project_id}\",\n",
    "        project_id = args.bq_project,\n",
    "        table_id = args.bq_table,\n",
    "        dataset_id = args.bq_dataset,\n",
    "        selected_fields = selected_fields,\n",
    "        output_types = output_types,\n",
    "        row_restriction = f\"splits='{split}'\",\n",
    "        requested_streams = 3\n",
    "    )\n",
    "    \n",
    "    return training\n",
    "\n",
    "# setup feed for train, validate and test\n",
    "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
    "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1156e18-4f13-4f8c-99c9-5cd0449f262f",
   "metadata": {},
   "source": [
    "### Define The Model\n",
    "\n",
    "Define a DNN version of Logistic Regression using Keras sequential layers.\n",
    "\n",
    "#### Hyperparameter Tuning Specific Additions\n",
    "- the `tf.keras.optimizers.SGD` in parameterized with the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c42417ec-7f1a-4053-be82-f7c4ed406043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "# model input definitions\n",
    "feature_columns = {header: tf.feature_column.numeric_column(header) for header in selected_fields if header != args.var_target}\n",
    "feature_layer_inputs = {header: tf.keras.layers.Input(shape = (1,), name = header) for header in selected_fields if header != args.var_target}\n",
    "\n",
    "# feature columns to a Dense Feature Layer\n",
    "feature_layer_outputs = tf.keras.layers.DenseFeatures(feature_columns.values(), name = 'feature_layer')(feature_layer_inputs)\n",
    "\n",
    "# batch normalization then Dense with softmax activation to nclasses\n",
    "layers = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(feature_layer_outputs)\n",
    "layers = tf.keras.layers.Dense(64, activation = 'relu', name = 'hidden_layer')(layers)\n",
    "layers = tf.keras.layers.Dense(32, activation = 'relu', name = 'embedding_layer')(layers)\n",
    "layers = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'prediction_layer')(layers)\n",
    "\n",
    "# the model\n",
    "model = tf.keras.Model(\n",
    "    inputs = feature_layer_inputs,\n",
    "    outputs = layers,\n",
    "    name = args.experiment\n",
    ")\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = args.learning_rate, momentum = args.momentum) #SGD or Adam\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(\n",
    "    optimizer = opt,\n",
    "    loss = loss,\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(curve='PR', name = 'auprc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78611be3-f867-415f-b874-d00e2531b11d",
   "metadata": {},
   "source": [
    "### Train The Model\n",
    "Initialize the model fit or training and log the training information to Vertex AI Tensorboard using the predefinied environment variable `AIP_TENSORBOARD_LOG_DIR`.  When done, also log the training history to Vertex AI Experiments.\n",
    "\n",
    "#### Hyperparameter Tuning Specific Additions\n",
    "- an additional callback is defined, `hparams_callback` that writes the `hparams` setup created above to the callbacks location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73bf6cb8-d9e8-4aa8-8ad3-cb7ac978de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# setup tensorboard logs and train\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
    "hparams_callback = hp.KerasCallback(os.environ['AIP_TENSORBOARD_LOG_DIR'] + 'train/', hparams, trial_id = args.run_name)\n",
    "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback, hparams_callback], validation_data = validate)\n",
    "expRun.log_params({'epochs': history.params['epochs']})\n",
    "for e in range(0, history.params['epochs']):\n",
    "    expRun.log_time_series_metrics(\n",
    "        {\n",
    "            'train_loss': history.history['loss'][e],\n",
    "            'train_accuracy': history.history['accuracy'][e],\n",
    "            'train_auprc': history.history['auprc'][e],\n",
    "            'val_loss': history.history['val_loss'][e],\n",
    "            'val_accuracy': history.history['val_accuracy'][e],\n",
    "            'val_auprc': history.history['val_auprc'][e]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211e548-c08a-4a08-86d5-ac40158b983a",
   "metadata": {},
   "source": [
    "### Evaluate The Model\n",
    "Gather metrics for the trained model on each split of the data: train, validate, and test sets.\n",
    "\n",
    "#### Hyperparameter Tuning Specific Additions\n",
    "- the validation sets value of `auprc` is reported back to the hyperparameter tuning interface using the client.  This is then used in selecting future trials values of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cd89d2f-19ec-4129-a486-68ae2725b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# test evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(test)\n",
    "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
    "\n",
    "# val evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(validate)\n",
    "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
    "# report hypertune info back to Vertex AI Training > Hyperparamter Tuning Job\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag = 'auprc',\n",
    "    metric_value = history.history['auprc'][-1])\n",
    "\n",
    "# training evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(train)\n",
    "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90faaad5-2849-4329-bcfe-d5f2d03a09c2",
   "metadata": {},
   "source": [
    "### Save The Model\n",
    "Write the model save file to the GCS using the predefined environment variable `AIP_MODEL_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "531d45c8-d57c-4363-aab8-17cb1cf1c4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/hp_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# output the model save files\n",
    "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
    "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
    "expRun.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27081456-cd9c-41ba-9653-18b55a019bcb",
   "metadata": {},
   "source": [
    "---\n",
    "## Review The Training Code\n",
    "Read in the completed script and print it out for review below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45b54411-7840-4c42-a35f-b9771cdd5c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "\n",
       "# package import\n",
       "from tensorflow.python.framework import dtypes\n",
       "from tensorflow_io.bigquery import BigQueryClient\n",
       "import tensorflow as tf\n",
       "from google.cloud import bigquery\n",
       "from google.cloud import aiplatform\n",
       "import argparse\n",
       "import os\n",
       "import sys\n",
       "import hypertune\n",
       "from tensorboard.plugins.hparams import api as hp\n",
       "\n",
       "# import argument to local variables\n",
       "parser = argparse.ArgumentParser()\n",
       "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
       "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
       "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
       "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
       "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
       "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
       "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
       "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
       "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
       "parser.add_argument('--region', dest = 'region', type=str)\n",
       "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
       "parser.add_argument('--series', dest = 'series', type=str)\n",
       "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
       "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
       "# hyperparameters\n",
       "parser.add_argument('--lr', dest='learning_rate', required=True, type=float, help='Learning Rate')\n",
       "parser.add_argument('--m', dest='momentum', required=True, type=float, help='Momentum')\n",
       "args = parser.parse_args()\n",
       "\n",
       "# setup tensorboard hparams\n",
       "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.0, 1.0))\n",
       "HP_MOMENTUM = hp.HParam('momentum', hp.RealInterval(0.0,1.0))\n",
       "hparams = {\n",
       "    HP_LEARNING_RATE: args.learning_rate,\n",
       "    HP_MOMENTUM: args.momentum\n",
       "}\n",
       "\n",
       "# clients\n",
       "bq = bigquery.Client(project = args.project_id)\n",
       "aiplatform.init(project = args.project_id, location = args.region)\n",
       "hpt = hypertune.HyperTune()\n",
       "args.run_name = f'{args.run_name}-{hpt.trial_id}'\n",
       "\n",
       "# Vertex AI Experiment\n",
       "expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
       "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})\n",
       "expRun.log_params({'hyperparameter.learning_rate': args.learning_rate, 'hyperparameter.momentum': args.momentum})\n",
       "\n",
       "# get schema from bigquery source\n",
       "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
       "schema = bq.query(query).to_dataframe()\n",
       "\n",
       "# get number of classes from bigquery source\n",
       "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
       "nclasses = nclasses.shape[0]\n",
       "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
       "\n",
       "# Make a list of columns to omit\n",
       "OMIT = args.var_omit + ['splits']\n",
       "\n",
       "# use schema to prepare a list of columns to read from BigQuery\n",
       "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
       "\n",
       "# all the columns in this data source are either float64 or int64\n",
       "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]\n",
       "\n",
       "# remap input data to Tensorflow inputs of features and target\n",
       "def transTable(row_dict):\n",
       "    target = row_dict.pop(args.var_target)\n",
       "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
       "    target = tf.cast(target, tf.float32)\n",
       "    return(row_dict, target)\n",
       "\n",
       "# function to setup a bigquery reader with Tensorflow I/O\n",
       "def bq_reader(split):\n",
       "    reader = BigQueryClient()\n",
       "\n",
       "    training = reader.read_session(\n",
       "        parent = f\"projects/{args.project_id}\",\n",
       "        project_id = args.bq_project,\n",
       "        table_id = args.bq_table,\n",
       "        dataset_id = args.bq_dataset,\n",
       "        selected_fields = selected_fields,\n",
       "        output_types = output_types,\n",
       "        row_restriction = f\"splits='{split}'\",\n",
       "        requested_streams = 3\n",
       "    )\n",
       "    \n",
       "    return training\n",
       "\n",
       "# setup feed for train, validate and test\n",
       "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
       "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})\n",
       "\n",
       "# Logistic Regression\n",
       "\n",
       "# model input definitions\n",
       "feature_columns = {header: tf.feature_column.numeric_column(header) for header in selected_fields if header != args.var_target}\n",
       "feature_layer_inputs = {header: tf.keras.layers.Input(shape = (1,), name = header) for header in selected_fields if header != args.var_target}\n",
       "\n",
       "# feature columns to a Dense Feature Layer\n",
       "feature_layer_outputs = tf.keras.layers.DenseFeatures(feature_columns.values(), name = 'feature_layer')(feature_layer_inputs)\n",
       "\n",
       "# batch normalization then Dense with softmax activation to nclasses\n",
       "layers = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(feature_layer_outputs)\n",
       "layers = tf.keras.layers.Dense(64, activation = 'relu', name = 'hidden_layer')(layers)\n",
       "layers = tf.keras.layers.Dense(32, activation = 'relu', name = 'embedding_layer')(layers)\n",
       "layers = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'prediction_layer')(layers)\n",
       "\n",
       "# the model\n",
       "model = tf.keras.Model(\n",
       "    inputs = feature_layer_inputs,\n",
       "    outputs = layers,\n",
       "    name = args.experiment\n",
       ")\n",
       "opt = tf.keras.optimizers.SGD(learning_rate = args.learning_rate, momentum = args.momentum) #SGD or Adam\n",
       "loss = tf.keras.losses.CategoricalCrossentropy()\n",
       "model.compile(\n",
       "    optimizer = opt,\n",
       "    loss = loss,\n",
       "    metrics = ['accuracy', tf.keras.metrics.AUC(curve='PR', name = 'auprc')]\n",
       ")\n",
       "\n",
       "# setup tensorboard logs and train\n",
       "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
       "hparams_callback = hp.KerasCallback(os.environ['AIP_TENSORBOARD_LOG_DIR'] + 'train/', hparams, trial_id = args.run_name)\n",
       "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback, hparams_callback], validation_data = validate)\n",
       "expRun.log_params({'epochs': history.params['epochs']})\n",
       "for e in range(0, history.params['epochs']):\n",
       "    expRun.log_time_series_metrics(\n",
       "        {\n",
       "            'train_loss': history.history['loss'][e],\n",
       "            'train_accuracy': history.history['accuracy'][e],\n",
       "            'train_auprc': history.history['auprc'][e],\n",
       "            'val_loss': history.history['val_loss'][e],\n",
       "            'val_accuracy': history.history['val_accuracy'][e],\n",
       "            'val_auprc': history.history['val_auprc'][e]\n",
       "        }\n",
       "    )\n",
       "\n",
       "# test evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(test)\n",
       "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
       "\n",
       "# val evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(validate)\n",
       "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
       "# report hypertune info back to Vertex AI Training > Hyperparamter Tuning Job\n",
       "hpt.report_hyperparameter_tuning_metric(\n",
       "    hyperparameter_metric_tag = 'auprc',\n",
       "    metric_value = history.history['auprc'][-1])\n",
       "\n",
       "# training evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(train)\n",
       "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})\n",
       "\n",
       "# output the model save files\n",
       "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
       "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
       "expRun.end_run()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```python\\n\\n{data}\\n```\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
